{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn\")\n",
    "sns.set(font_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/train.csv')\n",
    "df_test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = [\"var_108\",\"var_34\",\"var_9\",\"var_94\",\"var_127\",\"var_13\",\"var_123\",\"var_1\",\"var_80\",\"var_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ext_feat(list_, df_):\n",
    "    for i, values in enumerate(var_list):\n",
    "        if i<(len(var_list)-1):\n",
    "            df[var_list[i]+\"+\"+var_list[i+1]] = df[var_list[i]] + df[var_list[i+1]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+1]] = df[var_list[i]] - df[var_list[i+1]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+1]] = df[var_list[i]] * df[var_list[i+1]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+1]] = df[var_list[i]] / df[var_list[i+1]]\n",
    "        if i<(len(var_list)-2):\n",
    "            df[var_list[i]+\"+\"+var_list[i+2]] = df[var_list[i]] + df[var_list[i+2]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+2]] = df[var_list[i]] - df[var_list[i+2]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+2]] = df[var_list[i]] * df[var_list[i+2]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+2]] = df[var_list[i]] / df[var_list[i+2]]\n",
    "        if i<(len(var_list)-3):\n",
    "            df[var_list[i]+\"+\"+var_list[i+3]] = df[var_list[i]] + df[var_list[i+3]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+3]] = df[var_list[i]] - df[var_list[i+3]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+3]] = df[var_list[i]] * df[var_list[i+3]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+3]] = df[var_list[i]] / df[var_list[i+3]]\n",
    "        if i<(len(var_list)-4):\n",
    "            df[var_list[i]+\"+\"+var_list[i+4]] = df[var_list[i]] + df[var_list[i+4]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+4]] = df[var_list[i]] - df[var_list[i+4]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+4]] = df[var_list[i]] * df[var_list[i+4]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+4]] = df[var_list[i]] / df[var_list[i+4]]\n",
    "        if i<(len(var_list)-5):\n",
    "            df[var_list[i]+\"+\"+var_list[i+5]] = df[var_list[i]] + df[var_list[i+5]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+5]] = df[var_list[i]] - df[var_list[i+5]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+5]] = df[var_list[i]] * df[var_list[i+5]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+5]] = df[var_list[i]] / df[var_list[i+5]]\n",
    "        if i<(len(var_list)-6):\n",
    "            df[var_list[i]+\"+\"+var_list[i+6]] = df[var_list[i]] + df[var_list[i+6]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+6]] = df[var_list[i]] - df[var_list[i+6]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+6]] = df[var_list[i]] * df[var_list[i+6]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+6]] = df[var_list[i]] / df[var_list[i+6]]\n",
    "        if i<(len(var_list)-7):\n",
    "            df[var_list[i]+\"+\"+var_list[i+7]] = df[var_list[i]] + df[var_list[i+7]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+7]] = df[var_list[i]] - df[var_list[i+7]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+7]] = df[var_list[i]] * df[var_list[i+7]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+7]] = df[var_list[i]] / df[var_list[i+7]]\n",
    "        if i<(len(var_list)-8):\n",
    "            df[var_list[i]+\"+\"+var_list[i+8]] = df[var_list[i]] + df[var_list[i+8]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+8]] = df[var_list[i]] - df[var_list[i+8]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+8]] = df[var_list[i]] * df[var_list[i+8]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+8]] = df[var_list[i]] / df[var_list[i+8]]\n",
    "        if i<(len(var_list)-9):\n",
    "            df[var_list[i]+\"+\"+var_list[i+9]] = df[var_list[i]] + df[var_list[i+9]]\n",
    "            df[var_list[i]+\"-\"+var_list[i+9]] = df[var_list[i]] - df[var_list[i+9]]\n",
    "            df[var_list[i]+\"*\"+var_list[i+9]] = df[var_list[i]] * df[var_list[i+9]]\n",
    "            df[var_list[i]+\"/\"+var_list[i+9]] = df[var_list[i]] / df[var_list[i+9]]\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ext_feat(var_list, df_train)\n",
    "df_test = ext_feat(var_list, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state = 42\n",
    "# np.random.seed(random_state)\n",
    "\n",
    "def augment(x,y,t=2):\n",
    "    xs,xn = [],[]\n",
    "    for i in range(t):\n",
    "        mask = y>0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xs.append(x1)\n",
    "\n",
    "    for i in range(t//2):\n",
    "        mask = y==0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        for c in range(x1.shape[1]):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:,c] = x1[ids][:,c]\n",
    "        xn.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x,xs,xn])\n",
    "    y = np.concatenate([y,ys,yn])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    \"objective\" : \"binary\",\n",
    "    \"metric\" : \"auc\",\n",
    "    \"boosting\": 'gbdt',\n",
    "    \"max_depth\" : -1,\n",
    "    \"num_leaves\" : 13,\n",
    "    \"learning_rate\" : 0.0083,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"bagging_fraction\" : 0.335,\n",
    "    \"feature_fraction\" : 0.041,\n",
    "    \"min_data_in_leaf\": 80,\n",
    "#     'subsample': 0.85,\n",
    "#     'min_child_weight': 1.5,\n",
    "#     'num_leaves': 2 ** 5,\n",
    "#     'colsample_bytree': 0.8,\n",
    "    \"min_sum_heassian_in_leaf\": 10,\n",
    "    \"tree_learner\": \"serial\",\n",
    "    \"boost_from_average\": \"false\",\n",
    "#     \"lambda_l1\" : 5,\n",
    "#     \"lambda_l2\" : 5,\n",
    "#     \"bagging_seed\" : random_state,\n",
    "    \"verbosity\" : 1,\n",
    "#     \"seed\": random_state,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=13, shuffle=True)\n",
    "oof = df_train[['ID_code', 'target']]\n",
    "oof['predict'] = 0\n",
    "predictions = df_test[['ID_code']]\n",
    "val_aucs = []\n",
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [col for col in df_train.columns if col not in ['target', 'ID_code']]\n",
    "X_test = df_test[features].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[1000]\ttraining's auc: 0.891705\tvalid_1's auc: 0.889905\n",
      "[2000]\ttraining's auc: 0.899379\tvalid_1's auc: 0.895426\n",
      "[3000]\ttraining's auc: 0.904493\tvalid_1's auc: 0.898882\n",
      "[4000]\ttraining's auc: 0.908256\tvalid_1's auc: 0.901246\n",
      "[5000]\ttraining's auc: 0.91115\tvalid_1's auc: 0.902823\n",
      "[6000]\ttraining's auc: 0.913623\tvalid_1's auc: 0.904059\n",
      "[7000]\ttraining's auc: 0.915749\tvalid_1's auc: 0.904877\n",
      "[8000]\ttraining's auc: 0.917658\tvalid_1's auc: 0.905517\n",
      "[9000]\ttraining's auc: 0.91947\tvalid_1's auc: 0.905844\n",
      "[10000]\ttraining's auc: 0.921153\tvalid_1's auc: 0.906112\n",
      "[11000]\ttraining's auc: 0.922764\tvalid_1's auc: 0.906404\n",
      "[12000]\ttraining's auc: 0.924337\tvalid_1's auc: 0.906421\n",
      "[13000]\ttraining's auc: 0.925858\tvalid_1's auc: 0.906286\n",
      "[14000]\ttraining's auc: 0.92736\tvalid_1's auc: 0.906313\n",
      "[15000]\ttraining's auc: 0.928846\tvalid_1's auc: 0.906287\n",
      "[16000]\ttraining's auc: 0.930277\tvalid_1's auc: 0.9063\n",
      "Early stopping, best iteration is:\n",
      "[12045]\ttraining's auc: 0.92441\tvalid_1's auc: 0.906429\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[1000]\ttraining's auc: 0.892045\tvalid_1's auc: 0.889829\n",
      "[2000]\ttraining's auc: 0.900065\tvalid_1's auc: 0.895706\n",
      "[3000]\ttraining's auc: 0.90507\tvalid_1's auc: 0.89865\n",
      "[4000]\ttraining's auc: 0.908798\tvalid_1's auc: 0.900854\n",
      "[5000]\ttraining's auc: 0.911657\tvalid_1's auc: 0.902481\n",
      "[6000]\ttraining's auc: 0.914106\tvalid_1's auc: 0.903652\n",
      "[7000]\ttraining's auc: 0.916225\tvalid_1's auc: 0.904438\n",
      "[8000]\ttraining's auc: 0.918118\tvalid_1's auc: 0.905027\n",
      "[9000]\ttraining's auc: 0.919884\tvalid_1's auc: 0.905225\n",
      "[10000]\ttraining's auc: 0.921583\tvalid_1's auc: 0.905483\n",
      "[11000]\ttraining's auc: 0.923174\tvalid_1's auc: 0.90571\n",
      "[12000]\ttraining's auc: 0.924722\tvalid_1's auc: 0.905756\n",
      "[13000]\ttraining's auc: 0.926254\tvalid_1's auc: 0.905764\n",
      "[14000]\ttraining's auc: 0.927723\tvalid_1's auc: 0.905897\n",
      "[15000]\ttraining's auc: 0.929179\tvalid_1's auc: 0.905937\n",
      "[16000]\ttraining's auc: 0.930603\tvalid_1's auc: 0.905849\n",
      "[17000]\ttraining's auc: 0.93202\tvalid_1's auc: 0.905668\n",
      "[18000]\ttraining's auc: 0.93341\tvalid_1's auc: 0.905719\n",
      "Early stopping, best iteration is:\n",
      "[14767]\ttraining's auc: 0.928836\tvalid_1's auc: 0.905958\n",
      "Training until validation scores don't improve for 4000 rounds.\n",
      "[1000]\ttraining's auc: 0.89198\tvalid_1's auc: 0.889464\n",
      "[2000]\ttraining's auc: 0.899934\tvalid_1's auc: 0.895279\n",
      "[3000]\ttraining's auc: 0.904863\tvalid_1's auc: 0.898799\n",
      "[4000]\ttraining's auc: 0.90857\tvalid_1's auc: 0.901283\n"
     ]
    }
   ],
   "source": [
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n",
    "    X_train, y_train = df_train.iloc[trn_idx][features], df_train.iloc[trn_idx]['target']\n",
    "    X_valid, y_valid = df_train.iloc[val_idx][features], df_train.iloc[val_idx]['target']\n",
    "    \n",
    "    N = 3\n",
    "    p_valid,yp = 0,0\n",
    "    for i in range(N):\n",
    "        X_t, y_t = augment(X_train.values, y_train.values)\n",
    "        X_t = pd.DataFrame(X_t)\n",
    "        X_t = X_t.add_prefix('var_')\n",
    "    \n",
    "        trn_data = lgb.Dataset(X_t, label=y_t)\n",
    "        val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        evals_result = {}\n",
    "        lgb_clf = lgb.train(lgb_params,\n",
    "                        trn_data,\n",
    "                        1000000,\n",
    "                        valid_sets = [trn_data, val_data],\n",
    "                        early_stopping_rounds=4000,\n",
    "                        verbose_eval = 1000,\n",
    "                        evals_result=evals_result\n",
    "                       )\n",
    "        p_valid += lgb_clf.predict(X_valid)\n",
    "        yp += lgb_clf.predict(X_test)\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    oof['predict'][val_idx] = p_valid/N\n",
    "    val_score = roc_auc_score(y_valid, p_valid)\n",
    "    val_aucs.append(val_score)\n",
    "    \n",
    "    predictions['fold{}'.format(fold+1)] = yp/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_auc = np.mean(val_aucs)\n",
    "std_auc = np.std(val_aucs)\n",
    "all_auc = roc_auc_score(oof['target'], oof['predict'])\n",
    "print(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]]\n",
    "        .groupby(\"feature\")\n",
    "        .mean()\n",
    "        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(14,26))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n",
    "plt.title('LightGBM Features (averaged over folds)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('lgbm_importances.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\n",
    "predictions.to_csv('lgb_all_predictions.csv', index=None)\n",
    "sub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\n",
    "sub_df[\"target\"] = predictions['target']\n",
    "sub_df.to_csv(\"lgb_submission.csv\", index=False)\n",
    "predictions[[\"ID_code\", \"target\"]].to_csv(\"../submission/lgb_oof\"+str(int(time.strftime(\"%Y%m%d%H%M%S\", time.localtime(time.time()))))+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
